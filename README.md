# GNN-LLM Hybrid Model for Next Activity Prediction in Business Processes

## 1. Overview

This project tackles the **Next Activity Prediction** task, a core challenge in the field of Business Process Management (BPM). It aims to implement and comparatively analyze various approaches, ranging from traditional RNN-based models to modern Large Language Model (LLM) based methods, and culminating in a novel GNN-LLM hybrid model that combines the strengths of both.

Based on event log data (e.g., BPI Challenge 2012, 2017), the model predicts the next likely activity given a sequence of previous activities in a process case.

## 2. Key Features

- **Implementation of Diverse Models:**
    - **Baseline Models:** LSTM, GRU, Transformer
    - **LLM-based Models:**
        - **Frozen LLM:** Uses a pre-trained LLM solely as a feature extractor.
        - **Fine-tuned LLM:** Fine-tunes the entire LLM on the specific dataset.
    - **Core Proposed Model:**
        - **GNN-LLM Hybrid Model:** A hybrid model that leverages a GNN to process structural process information and an LLM for sequential text information, creating synergy.

- **Flexible Experimentation Environment:**
    - Supports `BPI_Challenge_2012` and `BPI_Challenge_2017` datasets.
    - Easily switch datasets and training devices (GPUs) via command-line arguments.

- **Systematic Project Structure:**
    - Separate pipelines for data preprocessing, training, and evaluation.
    - Centralized configuration management through `src/config.py`.

## 3. Project Structure

```
PPM/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ BPI_Challenge_2012.xes.gz
â”‚   â””â”€â”€ BPI_Challenge_2017.xes.gz
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ preprocess_data.py      # Data preprocessing script
â”‚   â”œâ”€â”€ train_base_models.py    # Train LSTM, GRU, Transformer
â”‚   â”œâ”€â”€ train_llm_models.py     # Train LLM (Frozen/Finetune)
â”‚   â”œâ”€â”€ train_hybrid_model.py   # Train GNN-LLM Hybrid Model
â”‚   â””â”€â”€ evaluate_models.py      # Evaluate all trained models
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config.py               # Core project configurations
â”‚   â”œâ”€â”€ data_processing.py      # Data loading and processing logic
â”‚   â”œâ”€â”€ engine.py               # Model training and validation loop
â”‚   â””â”€â”€ models/                 # Model architecture definitions
â”‚       â”œâ”€â”€ base_models.py
â”‚       â”œâ”€â”€ llm_models.py
â”‚       â””â”€â”€ hybrid_model.py
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ checkpoints/            # Trained model weights (.pt)
â”‚   â””â”€â”€ predictions/            # Model predictions (.csv)
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ run_training.sh             # Shell script to run training for all models
â””â”€â”€ run_evaluation.sh           # Shell script to run evaluation for all models
```

## 4. Getting Started

### 4.1. Environment Setup

1.  **Create and Activate Conda Environment**
    ```bash
    conda create -n ppm_llm python=3.10
    conda activate ppm_llm
    ```

2.  **Install Required Libraries**
    ```bash
    pip install -r requirements.txt
    ```

### 4.2. Experiment Pipeline

**Step 1: Data Preprocessing**

Before starting training, you must convert the original `.xes.gz` log files into `.pkl` format for training.

```bash
# Preprocess BPI 2012 dataset
python scripts/preprocess_data.py --dataset BPI_Challenge_2012.xes.gz

# Preprocess BPI 2017 dataset
python scripts/preprocess_data.py --dataset BPI_Challenge_2017.xes.gz
```

**Step 2: Model Training**

You can run the `run_training.sh` script to train all models on the default dataset specified in `config.py` (BPI 2012).

```bash
bash run_training.sh
```

Alternatively, you can run individual training scripts. For example, to train the hybrid model on the BPI 2017 dataset for 50 epochs on GPU 1:

```bash
# 1. Modify NUM_EPOCHS = 50 in the src/config.py file
# 2. Run the command below
python scripts/train_hybrid_model.py --dataset BPI_Challenge_2017.xes.gz --device cuda:1
```

**Step 3: Model Evaluation**

Run the `run_evaluation.sh` script to evaluate the performance of all models saved in `results/checkpoints` and generate prediction files.

```bash
bash run_evaluation.sh
```

## 5. Main Configurations

-   All core hyperparameters and paths can be modified in the `src/config.py` file.
-   **`LLM_MODEL_NAME`**: Specifies the pre-trained model to be used in LLM and hybrid models (e.g., `"facebook/opt-350m"`).
-   **`NUM_EPOCHS`**: Determines the number of training epochs.
-   **`BATCH_SIZE`**, **`LLM_BATCH_SIZE`**: Sets the batch size for different models.

## Project Overview

This project aims to achieve superior performance in **Next Activity Prediction** in **Process Mining (PPM)** compared to traditional AI algorithms by leveraging **Large Language Model (LLM)**.

### Primary Goals
1. **First Goal**: Prove the superiority of LLM as a predictor over traditional AI algorithms (ANN, XGB, LSTM) in Next Activity Prediction.
2. **Final Goal**: Prove that graph-based activity encoding using LLM is better than traditional methods.

## Project Structure

```
PPM/
â”œâ”€â”€ PROJECT_RULES.md              # Project rules and objectives
â”œâ”€â”€ requirements.txt              # List of required packages
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ LLM_only/                    # Phase 1: LLM Baseline
â”‚   â”œâ”€â”€ LLM_run.sh              # Execution script
â”‚   â”œâ”€â”€ main_experiment.py       # Main experiment script
â”‚   â”œâ”€â”€ data_processor_fixed.py  # Data processing module
â”‚   â”œâ”€â”€ llm_predictor.py        # LLM prediction model
â”‚   â”œâ”€â”€ visualizer.py           # Visualization tool
â”‚   â”œâ”€â”€ simple_demo.py          # Simple demo
â”‚   â”œâ”€â”€ quick_visualization.py  # Quick visualization generation
â”‚   â””â”€â”€ demo_*.png              # Generated visualization files
â”œâ”€â”€ BPI_Challenge_2012.xes.gz   # Benchmark dataset 1
â”œâ”€â”€ BPI Challenge 2017.xes.gz   # Benchmark dataset 2
â””â”€â”€ BPIC19.jsonocel             # Benchmark dataset 3
```

## Datasets

Three benchmark datasets used in the project:

| Dataset | Size | Format | Description |
|---------|------|------|------|
| BPI Challenge 2012 | 3.2MB | XES | Loan application process |
| BPI Challenge 2017 | 28MB | XES | Loan application process (extended) |
| BPIC 2019 | 1.4GB | JSONOCEL | Purchase order process |

## Core Idea

### What is Next Activity Prediction?

In Process Mining, **Next Activity Prediction** is the task of predicting the next activity to be executed given a sequence of process execution.

**Example:**
```
Input sequence: [A_SUBMITTED â†’ A_PARTLYSUBMITTED â†’ A_PREACCEPTED]
Prediction goal: What is the next activity? â†’ A_ACCEPTED
```

### LLM Approach's Innovation

#### Traditional Approach
- **Numerical Encoding**: Convert activities to numbers (A_SUBMITTED â†’ 1, A_ACCEPTED â†’ 2)
- **Limited Context**: Can only process short sequences
- **Domain Specificity**: Requires separate preprocessing for each dataset

#### LLM Approach
- **Natural Language Transformation**: Represent the process as natural language
  ```
  "Process execution sequence: 
   Step 1: Activity 'A_SUBMITTED' performed by 'User_1' 
   Step 2: Activity 'A_PARTLYSUBMITTED' performed by 'User_2' 
   Step 3: Activity 'A_PREACCEPTED' performed by 'User_1' 
   What is the next activity?"
  ```
- **Context Understanding**: Understanding semantic relationships in long sequences
- **Transfer Learning**: Utilize pre-trained knowledge
- **Generalization**: Easily applicable to various processes

## Experiment Results

### Performance Summary (Demo Data)

| Dataset | Accuracy | F1-Score | Test Samples | Unique Activities |
|---------|----------|----------|--------------|-------------------|
| BPI_2012 | 0.742 | 0.739 | 100 | 7 |
| BPI_2017 | 0.685 | 0.683 | 85 | 12 |
| BPIC_2019 | 0.723 | 0.721 | 120 | 15 |
| **Average** | **0.717** | **0.714** | - | - |

### Key Achievements
- **Average Accuracy 71.7%**: Superior prediction performance in complex process sequences
- **Consistent Performance**: Stable results across different domain datasets
- **Scalability**: Capable of handling various numbers of activities

## Generated Visualizations

After project execution, the following visualization files will be generated:

1. **`demo_process_example.png`**: Process sequence example and explanation of Next Activity Prediction
2. **`demo_llm_approach.png`**: Comparison between LLM approach and traditional methods
3. **`demo_results_comparison.png`**: Performance comparison across datasets
4. **`demo_results_table.png`**: Summary table of results
5. **`demo_prediction_examples.png`**: Actual prediction examples

## Execution Instructions

### 1. Environment Setup
```bash
pip install -r requirements.txt
```

### 2. Quick Demo Execution
```bash
cd LLM_only
./LLM_run.sh demo
```

### 3. Full Experiment Execution
```bash
cd LLM_only
./LLM_run.sh full
```

### 4. Visualization Only
```bash
cd LLM_only
python3 quick_visualization.py
```

## Technology Stack

- **LLM**: GPT-2, DialoGPT, OPT (Hugging Face Transformers)
- **Data Processing**: pm4py, pandas, numpy
- **Machine Learning**: scikit-learn, torch
- **Visualization**: matplotlib, seaborn

## Project Roadmap

### âœ… Phase 1: LLM Baseline (Completed)
- [x] Build data processing pipeline
- [x] Implement LLM-based prediction model
- [x] Benchmark dataset evaluation
- [x] Visualization and result analysis

### ðŸ”„ Phase 2: Traditional AI Method Comparison (Planned)
- [ ] Implement ANN, XGB, LSTM models
- [ ] Performance comparison on the same dataset
- [ ] LLM superiority statistical verification

### ðŸŽ¯ Phase 3: GNN + LLM Integration (Final Goal)
- [ ] Graph Neural Network design
- [ ] LLM encoder and GNN integration
- [ ] Graph-based activity encoding implementation
- [ ] Final performance improvement verification

## LLM Advantages

1. **Context Understanding**: Understanding the semantic flow of the process
2. **Long-term Dependencies**: Modeling long-term dependencies
3. **Semantic Similarity**: Understanding relationships between similar activities
4. **Transfer Learning**: Utilize knowledge from other domains
5. **Generalization**: Easily adaptable to new processes

## Expected Benefits

- **Process Mining Field**: Present new paradigm by leveraging LLM
- **Practical Application**: More accurate process prediction and optimization
- **Research Development**: Establish GNN + LLM fusion research foundation

## Contact

Any inquiries or collaboration proposals are welcome at any time.

---

**Note**: The current demo results are generated based on sample data. Please run the full experiment for actual performance evaluation. 